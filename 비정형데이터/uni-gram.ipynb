{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhdqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dhdqu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_1.txt의 문서 분류 =trade 정확도 0.0%\n",
      "jobs_1.txt의 문서 분류 =jobs 정확도 50.0%\n",
      "money_supply_1.txt의 문서 분류 =trade 정확도 33.33333333333333%\n",
      "trade_1.txt의 문서 분류 =trade 정확도 50.0%\n",
      "interest_2.txt의 문서 분류 =interest 정확도 60.0%\n",
      "jobs_2.txt의 문서 분류 =jobs 정확도 66.66666666666666%\n",
      "money_supply_2.txt의 문서 분류 =money_supply 정확도 71.42857142857143%\n",
      "trade_2.txt의 문서 분류 =trade 정확도 75.0%\n",
      "interest_3.txt의 문서 분류 =interest 정확도 77.77777777777779%\n",
      "jobs_3.txt의 문서 분류 =jobs 정확도 80.0%\n",
      "money_supply_3.txt의 문서 분류 =money_supply 정확도 81.81818181818183%\n",
      "trade_3.txt의 문서 분류 =trade 정확도 83.33333333333334%\n",
      "interest_4.txt의 문서 분류 =interest 정확도 84.61538461538461%\n",
      "jobs_4.txt의 문서 분류 =jobs 정확도 85.71428571428571%\n",
      "money_supply_4.txt의 문서 분류 =money_supply 정확도 86.66666666666667%\n",
      "trade_4.txt의 문서 분류 =trade 정확도 87.5%\n",
      "interest_5.txt의 문서 분류 =trade 정확도 82.35294117647058%\n",
      "jobs_5.txt의 문서 분류 =jobs 정확도 83.33333333333334%\n",
      "money_supply_5.txt의 문서 분류 =money_supply 정확도 84.21052631578947%\n",
      "trade_5.txt의 문서 분류 =trade 정확도 85.0%\n",
      "interest_6.txt의 문서 분류 =interest 정확도 85.71428571428571%\n",
      "jobs_6.txt의 문서 분류 =jobs 정확도 86.36363636363636%\n",
      "money_supply_6.txt의 문서 분류 =money_supply 정확도 86.95652173913044%\n",
      "trade_6.txt의 문서 분류 =jobs 정확도 83.33333333333334%\n",
      "interest_7.txt의 문서 분류 =interest 정확도 84.0%\n",
      "jobs_7.txt의 문서 분류 =jobs 정확도 84.61538461538461%\n",
      "money_supply_7.txt의 문서 분류 =money_supply 정확도 85.18518518518519%\n",
      "trade_7.txt의 문서 분류 =trade 정확도 85.71428571428571%\n",
      "interest_8.txt의 문서 분류 =interest 정확도 86.20689655172413%\n",
      "jobs_8.txt의 문서 분류 =jobs 정확도 86.66666666666667%\n",
      "money_supply_8.txt의 문서 분류 =trade 정확도 83.87096774193549%\n",
      "trade_8.txt의 문서 분류 =trade 정확도 84.375%\n",
      "interest_9.txt의 문서 분류 =interest 정확도 84.84848484848484%\n",
      "jobs_9.txt의 문서 분류 =jobs 정확도 85.29411764705883%\n",
      "money_supply_9.txt의 문서 분류 =money_supply 정확도 85.71428571428571%\n",
      "trade_9.txt의 문서 분류 =trade 정확도 86.11111111111111%\n",
      "interest_10.txt의 문서 분류 =interest 정확도 86.48648648648648%\n",
      "jobs_10.txt의 문서 분류 =jobs 정확도 86.8421052631579%\n",
      "money_supply_10.txt의 문서 분류 =money_supply 정확도 87.17948717948718%\n",
      "trade_10.txt의 문서 분류 =trade 정확도 87.5%\n",
      "interest_11.txt의 문서 분류 =interest 정확도 87.8048780487805%\n",
      "jobs_11.txt의 문서 분류 =jobs 정확도 88.09523809523809%\n",
      "money_supply_11.txt의 문서 분류 =money_supply 정확도 88.37209302325581%\n",
      "trade_11.txt의 문서 분류 =trade 정확도 88.63636363636364%\n",
      "interest_12.txt의 문서 분류 =interest 정확도 88.88888888888889%\n",
      "jobs_12.txt의 문서 분류 =jobs 정확도 89.13043478260869%\n",
      "money_supply_12.txt의 문서 분류 =money_supply 정확도 89.36170212765957%\n",
      "trade_12.txt의 문서 분류 =trade 정확도 89.58333333333334%\n",
      "interest_13.txt의 문서 분류 =interest 정확도 89.79591836734694%\n",
      "jobs_13.txt의 문서 분류 =jobs 정확도 90.0%\n",
      "money_supply_13.txt의 문서 분류 =money_supply 정확도 90.19607843137256%\n",
      "trade_13.txt의 문서 분류 =trade 정확도 90.38461538461539%\n",
      "interest_14.txt의 문서 분류 =interest 정확도 90.56603773584906%\n",
      "jobs_14.txt의 문서 분류 =jobs 정확도 90.74074074074075%\n",
      "money_supply_14.txt의 문서 분류 =money_supply 정확도 90.9090909090909%\n",
      "trade_14.txt의 문서 분류 =trade 정확도 91.07142857142857%\n",
      "interest_15.txt의 문서 분류 =interest 정확도 91.22807017543859%\n",
      "jobs_15.txt의 문서 분류 =jobs 정확도 91.37931034482759%\n",
      "money_supply_15.txt의 문서 분류 =money_supply 정확도 91.52542372881356%\n",
      "trade_15.txt의 문서 분류 =trade 정확도 91.66666666666666%\n",
      "interest_16.txt의 문서 분류 =interest 정확도 91.80327868852459%\n",
      "jobs_16.txt의 문서 분류 =jobs 정확도 91.93548387096774%\n",
      "money_supply_16.txt의 문서 분류 =interest 정확도 90.47619047619048%\n",
      "trade_16.txt의 문서 분류 =trade 정확도 90.625%\n",
      "interest_17.txt의 문서 분류 =trade 정확도 89.23076923076924%\n",
      "jobs_17.txt의 문서 분류 =jobs 정확도 89.39393939393939%\n",
      "money_supply_17.txt의 문서 분류 =money_supply 정확도 89.55223880597015%\n",
      "trade_17.txt의 문서 분류 =trade 정확도 89.70588235294117%\n",
      "interest_18.txt의 문서 분류 =interest 정확도 89.85507246376811%\n",
      "jobs_18.txt의 문서 분류 =jobs 정확도 90.0%\n",
      "money_supply_18.txt의 문서 분류 =money_supply 정확도 90.14084507042254%\n",
      "trade_18.txt의 문서 분류 =trade 정확도 90.27777777777779%\n",
      "interest_19.txt의 문서 분류 =interest 정확도 90.41095890410958%\n",
      "jobs_19.txt의 문서 분류 =jobs 정확도 90.54054054054053%\n",
      "money_supply_19.txt의 문서 분류 =interest 정확도 89.33333333333333%\n",
      "trade_19.txt의 문서 분류 =trade 정확도 89.47368421052632%\n",
      "interest_20.txt의 문서 분류 =interest 정확도 89.6103896103896%\n",
      "jobs_20.txt의 문서 분류 =jobs 정확도 89.74358974358975%\n",
      "money_supply_20.txt의 문서 분류 =money_supply 정확도 89.87341772151899%\n",
      "trade_20.txt의 문서 분류 =trade 정확도 90.0%\n",
      "interest_21.txt의 문서 분류 =interest 정확도 90.12345679012346%\n",
      "jobs_21.txt의 문서 분류 =jobs 정확도 90.2439024390244%\n",
      "money_supply_21.txt의 문서 분류 =money_supply 정확도 90.36144578313254%\n",
      "trade_21.txt의 문서 분류 =trade 정확도 90.47619047619048%\n",
      "interest_22.txt의 문서 분류 =interest 정확도 90.58823529411765%\n",
      "jobs_22.txt의 문서 분류 =jobs 정확도 90.69767441860465%\n",
      "money_supply_22.txt의 문서 분류 =money_supply 정확도 90.80459770114942%\n",
      "trade_22.txt의 문서 분류 =trade 정확도 90.9090909090909%\n",
      "interest_23.txt의 문서 분류 =interest 정확도 91.01123595505618%\n",
      "jobs_23.txt의 문서 분류 =jobs 정확도 91.11111111111111%\n",
      "money_supply_23.txt의 문서 분류 =money_supply 정확도 91.20879120879121%\n",
      "trade_23.txt의 문서 분류 =trade 정확도 91.30434782608695%\n",
      "interest_24.txt의 문서 분류 =interest 정확도 91.39784946236558%\n",
      "jobs_24.txt의 문서 분류 =jobs 정확도 91.48936170212765%\n",
      "money_supply_24.txt의 문서 분류 =money_supply 정확도 91.57894736842105%\n",
      "trade_24.txt의 문서 분류 =trade 정확도 91.66666666666666%\n",
      "interest_25.txt의 문서 분류 =interest 정확도 91.75257731958763%\n",
      "jobs_25.txt의 문서 분류 =jobs 정확도 91.83673469387756%\n",
      "money_supply_25.txt의 문서 분류 =money_supply 정확도 91.91919191919192%\n",
      "trade_25.txt의 문서 분류 =trade 정확도 92.0%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import io\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import math\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "def word_probabilities(dic, sub_total):#우도 계산\n",
    "    for i in range(0, 4):\n",
    "        for w in dic[i]:\n",
    "            dic[i][w]=(word_list[i][w])/(sub_total[i])\n",
    "\n",
    "def get_log_value(test_data, dic, weight):#값 추출\n",
    "    ans=-2100000000\n",
    "    ans_result=0\n",
    "    for i in range(0, 4):\n",
    "        result=0\n",
    "        for w in test_data:\n",
    "            if w in dic[i]:\n",
    "               # print(w+\" \"+str(dic[i][w])+\" \"+str(test_data[w]))\n",
    "                result+=(math.log(dic[i][w], 10)*test_data[w])\n",
    "            else:\n",
    "                result+=math.log(weight, 10)\n",
    "        if (ans<result):\n",
    "            ans=result\n",
    "            ans_result=i\n",
    "    return ans_result\n",
    "\n",
    "nltk.download(\"stopwords\")#불용어 사전 다운로드\n",
    "nltk.download('averaged_perceptron_tagger')#이건 왜 다운받았더라..?\n",
    "stop_words=set(stopwords.words('english'))\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "average=0\n",
    "ans_cnt=0\n",
    "cnt=0\n",
    "\n",
    "for k in range(1, 26):\n",
    "    word_list=[]#word_list[0]= interest 단어 딕셔너리, word_list[1]=jobs 단어 딕셔너리, word_list[2]= money_supply 단어 딕셔너리, word_list[3]=trade 단어 딕셔너리\n",
    "    cartegory=[\"interest\", \"jobs\", \"money_supply\", \"trade\"]\n",
    "    total=0\n",
    "    sub_total=[]# 각 카테고리 별 단어의 횟수 총합\n",
    "    dic=[]#확률 계산을 담는 리스트형 딕셔너리\n",
    "    \n",
    "    for i in range(0, 4):\n",
    "        words={}\n",
    "        for j in range(1, 26):\n",
    "            if j==k:\n",
    "                continue\n",
    "            f=\"dataset\\\\dev\\\\\"+cartegory[i]+\"\\\\\"+cartegory[i]+\"_\"+str(j)+\".txt\"\n",
    "            sentence=\"\"\n",
    "            word_tokens=[]#tokenize를 담는 리스트\n",
    "            li=[]#불용어를 제외한 리스트\n",
    "            lemmatize=[]#표제어\n",
    "            compare=[]#표제어 한 단어들의 품사 태깅 정보\n",
    "        \n",
    "            with open(f, 'r') as train:\n",
    "                sentence=train.read()#파일 읽기\n",
    "                sentence=sentence.lower()#소문자로 치환\n",
    "                #특수문자, 숫자 제거\n",
    "                parse=re.sub(\"\"\"[=.#/?:$}()&;*^%@!+<>,\"']|[0-9]\"\"\", \"\", sentence)\n",
    "                #탭, 엔터 제거\n",
    "                parse=re.sub(\"[\\s]|[-]\", \" \", parse)\n",
    "                #공백 여러개 하나로 치환\n",
    "                parse=re.sub(\" +\", \" \", parse)\n",
    "                #토큰 분리\n",
    "                word_tokens=word_tokenize(parse)\n",
    "        \n",
    "            #불용어 제거\n",
    "            for w in word_tokens:\n",
    "                if w not in stop_words:\n",
    "                    li.append(w)\n",
    "        \n",
    "            #품사태깅\n",
    "            pos_li=nltk.pos_tag(li)\n",
    "        \n",
    "            #표제어화\n",
    "            for w in pos_li:\n",
    "                try:\n",
    "                    tag=get_wordnet_pos(w[1])\n",
    "                    lemmatize.append(lemmatizer.lemmatize(w[0], tag))\n",
    "                except Exception as e:\n",
    "                    lemmatize.append(\"TagExceptionError\")#tag가 none인 경우\n",
    "\n",
    "            compare=nltk.pos_tag(lemmatize)\n",
    "        \n",
    "            #품사 태깅을 통한 lemmatize\n",
    "            for (p, c) in zip (pos_li, compare):\n",
    "                if c[0]!=\"TagExceptionError\":#tag가 none이 아닌 경우\n",
    "                    tag1=get_wordnet_pos(p[1])\n",
    "                    tag2=get_wordnet_pos(c[1])\n",
    "                    if(tag1==tag2):#둘의 tag가 같은 경우\n",
    "                        if(c[0] in words):\n",
    "                            words[c[0]]+=1\n",
    "                        else:\n",
    "                            words[c[0]]=1\n",
    "                    elif(tag1=='v' and tag2=='n'):#표제어화 했을때 동사에서 명사로 바뀐경우\n",
    "                        if(c[0] in words):\n",
    "                            words[c[0]]+=1\n",
    "                        else:\n",
    "                            words[c[0]]=1\n",
    "                    elif(tag1=='v' and tag2=='r'):#표제어화 했을때 동사에서 형용사로 바뀐경우\n",
    "                        if(c[0] in words):\n",
    "                            words[c[0]]+=1\n",
    "                        else:\n",
    "                            words[c[0]]=1\n",
    "                    else:#둘의 tag가 다른경우 (ex> goods->good)\n",
    "                        if(p[0] in words):\n",
    "                            words[p[0]]+=1\n",
    "                        else:\n",
    "                            words[p[0]]=1\n",
    "                    \n",
    "        word_list.append(words)\n",
    "    \n",
    "    for i in range(0, 4):\n",
    "        tmp=0\n",
    "        sub_dic={}\n",
    "        for w in word_list[i]:\n",
    "            tmp+=word_list[i][w]\n",
    "            sub_dic[w]=0\n",
    "        sub_total.append(tmp)\n",
    "        dic.append(sub_dic)\n",
    "    \n",
    "    word_probabilities(dic, sub_total)\n",
    "    \n",
    "    test_data=[]\n",
    "    for i in range(0, 4):\n",
    "        test_words={}\n",
    "        test_file=\"dataset\\\\dev\\\\\"+cartegory[i]+\"\\\\\"+cartegory[i]+\"_\"+str(k)+\".txt\"\n",
    "        test_sentence=\"\"\n",
    "        test_word_tokens=[]\n",
    "        test_li=[]\n",
    "        test_lemmatize=[]\n",
    "        test_compare=[]\n",
    "        \n",
    "        with open(test_file, 'r') as test:\n",
    "            test_sentence=test.read()\n",
    "            test_sentence=test_sentence.lower()\n",
    "            test_parse=re.sub(\"\"\"[=.#/?:$}()&;*^%@!+<>,\"']|[0-9]\"\"\", \"\", test_sentence)\n",
    "            test_parse=re.sub(\"[\\s]|[-]\", \" \", test_parse)\n",
    "            #공백 여러개 하나로 치환\n",
    "            test_parse=re.sub(\" +\", \" \", test_parse)\n",
    "            #토큰 분리\n",
    "            test_word_tokens=word_tokenize(test_parse)\n",
    "        \n",
    "        for w in test_word_tokens:\n",
    "            if w not in stop_words:\n",
    "                test_li.append(w)\n",
    "        \n",
    "        test_pos_li=nltk.pos_tag(test_li)\n",
    "        \n",
    "        for w in test_pos_li:\n",
    "            try:\n",
    "                tag=get_wordnet_pos(w[1])\n",
    "                test_lemmatize.append(lemmatizer.lemmatize(w[0], tag))\n",
    "            except Exception as e:\n",
    "                test_lemmatize.append(\"TagExceptionError\")#tag가 none인 경우\n",
    "\n",
    "        test_compare=nltk.pos_tag(test_lemmatize)\n",
    "        \n",
    "        for (p, c) in zip (test_pos_li, test_compare):\n",
    "            if c[0]!=\"TagExceptionError\":#tag가 none이 아닌 경우\n",
    "                tag1=get_wordnet_pos(p[1])\n",
    "                tag2=get_wordnet_pos(c[1])\n",
    "                if(tag1==tag2):#둘의 tag가 같은 경우\n",
    "                    if(c[0] in test_words):\n",
    "                        test_words[c[0]]+=1\n",
    "                    else:\n",
    "                        test_words[c[0]]=1\n",
    "                elif(tag1=='v' and tag2=='n'):#표제어화 했을때 동사에서 명사로 바뀐경우\n",
    "                    if(c[0] in test_words):\n",
    "                        test_words[c[0]]+=1\n",
    "                    else:\n",
    "                        test_words[c[0]]=1\n",
    "                elif(tag1=='v' and tag2=='r'):#표제어화 했을때 동사에서 형용사로 바뀐경우\n",
    "                    if(c[0] in test_words):\n",
    "                        test_words[c[0]]+=1\n",
    "                    else:\n",
    "                        test_words[c[0]]=1\n",
    "                else:#둘의 tag가 다른경우 (ex> goods->good)\n",
    "                    if(p[0] in test_words):\n",
    "                        test_words[p[0]]+=1\n",
    "                    else:\n",
    "                        test_words[p[0]]=1\n",
    "                \n",
    "        test_data.append(test_words)\n",
    "    \n",
    "    for z in range(0, 4):\n",
    "        cnt+=1\n",
    "        tmp=get_log_value(test_data[z], dic, 0.0000000005)\n",
    "        if(tmp==z):\n",
    "            ans_cnt+=1\n",
    "        print(cartegory[z]+\"_\"+str(k)+\".txt의 문서 분류 =\"+cartegory[tmp]+\" 정확도 \"+str((ans_cnt/cnt)*100)+\"%\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 파일 interest\n",
      "2번 파일 money_supply\n",
      "3번 파일 interest\n",
      "4번 파일 jobs\n",
      "5번 파일 money_supply\n",
      "6번 파일 jobs\n",
      "7번 파일 trade\n",
      "8번 파일 jobs\n",
      "9번 파일 jobs\n",
      "10번 파일 interest\n",
      "11번 파일 money_supply\n",
      "12번 파일 trade\n",
      "13번 파일 money_supply\n",
      "14번 파일 trade\n",
      "15번 파일 money_supply\n",
      "16번 파일 trade\n",
      "17번 파일 interest\n",
      "18번 파일 trade\n",
      "19번 파일 interest\n",
      "20번 파일 trade\n"
     ]
    }
   ],
   "source": [
    "word_list=[]#word_list[0]= interest 단어 딕셔너리, word_list[1]=jobs 단어 딕셔너리, word_list[2]= money_supply 단어 딕셔너리, word_list[3]=trade 단어 딕셔너리\n",
    "cartegory=[\"interest\", \"jobs\", \"money_supply\", \"trade\"]\n",
    "total=0\n",
    "sub_total=[]# 각 카테고리 별 단어의 횟수 총합\n",
    "dic=[]#확률 계산을 담는 리스트형 딕셔너리\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 4):# 각 분야별로 전처리\n",
    "    words={}\n",
    "    for j in range(1, 26):\n",
    "        f=\"dataset\\\\dev\\\\\"+cartegory[i]+\"\\\\\"+cartegory[i]+\"_\"+str(j)+\".txt\"\n",
    "        sentence=\"\"\n",
    "        word_tokens=[]#tokenize를 담는 리스트\n",
    "        li=[]#불용어를 제외한 리스트\n",
    "        lemmatize=[]#표제어\n",
    "        compare=[]#표제어 한 단어들의 품사 태깅 정보\n",
    "        \n",
    "        with open(f, 'r') as train:\n",
    "            sentence=train.read()#파일 읽기\n",
    "            sentence=sentence.lower()#소문자로 치환\n",
    "            #특수문자, 숫자 제거\n",
    "            parse=re.sub(\"\"\"[=.#/?:$}()&;*^%@!+<>,\"']|[0-9]\"\"\", \"\", sentence)\n",
    "            #탭, 엔터 제거\n",
    "            parse=re.sub(\"[\\s]|[-]\", \" \", parse)\n",
    "            #공백 여러개 하나로 치환\n",
    "            parse=re.sub(\" +\", \" \", parse)\n",
    "            #토큰 분리\n",
    "            word_tokens=word_tokenize(parse)\n",
    "        \n",
    "        #불용어 제거\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                li.append(w)\n",
    "        \n",
    "        #품사태깅\n",
    "        pos_li=nltk.pos_tag(li)\n",
    "        \n",
    "        #표제어화\n",
    "        for w in pos_li:\n",
    "            try:\n",
    "                tag=get_wordnet_pos(w[1])\n",
    "                lemmatize.append(lemmatizer.lemmatize(w[0], tag))\n",
    "            except Exception as e:\n",
    "                lemmatize.append(\"TagExceptionError\")#tag가 none인 경우\n",
    "\n",
    "        compare=nltk.pos_tag(lemmatize)\n",
    "        \n",
    "        #품사 태깅을 통한 lemmatize\n",
    "        for (p, c) in zip (pos_li, compare):\n",
    "            if c[0]!=\"TagExceptionError\":#tag가 none이 아닌 경우\n",
    "                tag1=get_wordnet_pos(p[1])\n",
    "                tag2=get_wordnet_pos(c[1])\n",
    "                if(tag1==tag2):#둘의 tag가 같은 경우\n",
    "                    if(c[0] in words):\n",
    "                        words[c[0]]+=1\n",
    "                    else:\n",
    "                        words[c[0]]=1\n",
    "                elif(tag1=='v' and tag2=='n'):#표제어화 했을때 동사에서 명사로 바뀐경우\n",
    "                    if(c[0] in words):\n",
    "                        words[c[0]]+=1\n",
    "                    else:\n",
    "                        words[c[0]]=1\n",
    "                elif(tag1=='v' and tag2=='r'):#표제어화 했을때 동사에서 형용사로 바뀐경우\n",
    "                    if(c[0] in words):\n",
    "                        words[c[0]]+=1\n",
    "                    else:\n",
    "                        words[c[0]]=1\n",
    "                else:#둘의 tag가 다른경우 (ex> goods->good)\n",
    "                    if(p[0] in words):\n",
    "                        words[p[0]]+=1\n",
    "                    else:\n",
    "                        words[p[0]]=1\n",
    "\n",
    "    word_list.append(words)\n",
    "    \n",
    "for i in range(0, 4):\n",
    "    tmp=0\n",
    "    sub_dic={}\n",
    "    for w in word_list[i]:\n",
    "        tmp+=word_list[i][w]\n",
    "        sub_dic[w]=0\n",
    "    sub_total.append(tmp)\n",
    "    dic.append(sub_dic)\n",
    "\n",
    "word_probabilities(dic, sub_total)\n",
    "test_data=[]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    words={}\n",
    "    f=\"dataset\\\\test\\\\\"+str(i)+\".txt\"\n",
    "    sentence=\"\"\n",
    "    word_tokens=[]#tokenize를 담는 리스트\n",
    "    li=[]#불용어를 제외한 리스트\n",
    "    lemmatize=[]#표제어\n",
    "    compare=[]#표제어 한 단어들의 품사 태깅 정보\n",
    "        \n",
    "    with open(f, 'r') as train:\n",
    "        sentence=train.read()#파일 읽기\n",
    "        sentence=sentence.lower()#소문자로 치환\n",
    "        #특수문자, 숫자 제거\n",
    "        parse=re.sub(\"\"\"[=.#/?:$}()&;*^%@!+<>,\"']|[0-9]\"\"\", \"\", sentence)\n",
    "        #탭, 엔터 제거\n",
    "        parse=re.sub(\"[\\s]|[-]\", \" \", parse)\n",
    "        #공백 여러개 하나로 치환\n",
    "        parse=re.sub(\" +\", \" \", parse)\n",
    "        #토큰 분리\n",
    "        word_tokens=word_tokenize(parse)\n",
    "    #불용어 제거\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            li.append(w)\n",
    "        \n",
    "    #품사태깅\n",
    "    pos_li=nltk.pos_tag(li)\n",
    "        \n",
    "    #표제어화\n",
    "    for w in pos_li:\n",
    "        try:\n",
    "            tag=get_wordnet_pos(w[1])\n",
    "            lemmatize.append(lemmatizer.lemmatize(w[0], tag))\n",
    "        except Exception as e:\n",
    "            lemmatize.append(\"TagExceptionError\")#tag가 none인 경우\n",
    "\n",
    "    compare=nltk.pos_tag(lemmatize)\n",
    "        \n",
    "    #품사 태깅을 통한 lemmatize\n",
    "    for (p, c) in zip (pos_li, compare):\n",
    "        if c[0]!=\"TagExceptionError\":#tag가 none이 아닌 경우\n",
    "            tag1=get_wordnet_pos(p[1])\n",
    "            tag2=get_wordnet_pos(c[1])\n",
    "            if(tag1==tag2):#둘의 tag가 같은 경우\n",
    "                if(c[0] in words):\n",
    "                    words[c[0]]+=1\n",
    "                else:\n",
    "                    words[c[0]]=1\n",
    "            elif(tag1=='v' and tag2=='n'):#표제어화 했을때 동사에서 명사로 바뀐경우\n",
    "                if(c[0] in words):\n",
    "                    words[c[0]]+=1\n",
    "                else:\n",
    "                    words[c[0]]=1\n",
    "            elif(tag1=='v' and tag2=='r'):#표제어화 했을때 동사에서 형용사로 바뀐경우\n",
    "                if(c[0] in words):\n",
    "                    words[c[0]]+=1\n",
    "                else:\n",
    "                    words[c[0]]=1\n",
    "            else:#둘의 tag가 다른경우 (ex> goods->good)\n",
    "                if(p[0] in words):\n",
    "                    words[p[0]]+=1\n",
    "                else:\n",
    "                    words[p[0]]=1\n",
    "    test_data.append(words)\n",
    "\n",
    "for z in range(1, 21):\n",
    "    print(str(z)+\"번 파일 \"+cartegory[get_log_value(test_data[z-1], dic, 0.0000000005)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
